[{"categories":["molecular biology","metascience"],"contents":"Molecular cloning: it is a ubiquitous and essential molecular biology technique, yet it’s the bane of many a scientist\u0026rsquo;s existence. If you\u0026rsquo;ve ever overheard a biologist muttering curses at a petri dish, chances are their cloning has gone wrong.\nDon\u0026rsquo;t believe me? Let\u0026rsquo;s take a peek at the r/labrats subreddit, where desperate researchers bare their souls:\n“For cloning, the most reliable method is a contract with a demon, higher the better.”\n\u0026ldquo;But of course the gods of cloning have different ideas about my free time. None of the reactions, even the control one, succeed.”\n“Cloning, like most of molecular biology is black magic. I\u0026rsquo;ve never met more superstitious people then molecular biologists, and seemingly the more superstitious the better they were in the lab.”\nNothing says \u0026ldquo;doing science\u0026rdquo; quite like demonic bargaining, right?\nEven when you think you’ve emerged triumphant from your cloning, there’s a good chance you haven’t. In a recent analysis of 2,521 plasmids constructed by researchers and sent to molecular cloning company VectorBuilder, nearly half contained errors compared to the reference sequence. Abysmal. (if you’re reading this and know of other interesting cloning related studies, let me know!)\nTo be fair, cloning isn’t the only dark art in molecular biology. One study finds human-designed PCR fails 37-45% of the time. A study investigating how well 2165 recombinant human proteins express in hamster cells found that 41% of human proteins did not have detectable expression. Perhaps I will think more extensively about these other methods in the future. But I suspect learning from cloning will provide insight into these others too.\nWhy does cloning make biologists contemplate career changes? And does it have to be this way?\nCloning 101 First, a little background. Molecular cloning is the process of assembling pieces of DNA into a circle (a plasmid) that can replicate inside a host organism. It\u0026rsquo;s the first step in many critical tasks, from protein production to gene editing. The process looks like this:.\nDesign your plasmid Assemble DNA fragments Transform host cells with the assembled DNA Identify clonal colonies with the correctly assembled plasmid End to end, this process should take just a few days. Emphasis on should.\nPlagued by uninformative latent failure Unfortunately, cloning outcomes can be quite variable, even for experienced practitioners. Sometimes it works flawlessly on the first try, leaving you feeling like a real scientist. Other times, it works once but refuses to cooperate ever again. And then there are those dark times when it stubbornly fails, again and again for weeks, leaving you with no choice but to sacrifice your first-born child. Wait, are we still talking about cloning?\nWhen cloning fails, it\u0026rsquo;s not very forthcoming about why. There are only a few observable failure states:\nCells with incorrect plasmids (misassembled, mutated, or containing part/backbone plasmids) No cells growing at all That\u0026rsquo;s all the feedback you get. Each failure could be the result of any number of different errors. I call this uninformative failure.\nAnother thing to notice is that failure is observed far after a potential error is made. This is called latent failure. You have to carry out the whole process to see if you did something wrong, making debugging cycles take forever.\nLatent failure of cloning Debugging execution and design errors So what do you do when your cloning has failed? The typical advice is:\nDo it again (and add a positive control this time, you rookie) Order new reagents or use a different kit (because clearly, it\u0026rsquo;s the reagents\u0026rsquo; fault, not yours) Check your fragments and regenerate your input sample (just in case) Trust nobody (not even yourself) With experience, you become better at catching execution errors, and you can develop intuitions which can help you narrow down what follow up conditions to try.\nBut even if you execute protocols perfectly, your cloning can still fail. Why? Because sometimes, the very DNA you\u0026rsquo;re trying to clone is the problem.\nCloning involves introducing new DNA sequences into a host organism. But the protein you\u0026rsquo;re trying to clone might be toxic to the host, or the DNA might contain hidden elements that produce unexpected, harmful products (called cryptic genetic elements), or the plasmid may contain sequences such as repeats that make it unstable. Even for hosts like E. coli, we can\u0026rsquo;t always predict whether it will tolerate a particular DNA sequence.\nIn lieu of perfect predictability, scientists have come up with many \u0026ldquo;rules\u0026rdquo; for designing plasmids and DNA fragments. Use tightly regulated expression systems! Reduce repetitive regions! Use efficient overhangs! For beginners to cloning, understanding why these rules exist by unintentionally violating them is almost a rite of passage. I know it was for me.\nThe problem? Not all, but many rules are context- and host-dependent and only become apparent after years of experience. Current plasmid design systems (Benchling, SnapGene) incorporate some basic rule-checking, but they can\u0026rsquo;t account for the diversity of sequences that scientists want to clone. Instead, the state of the art in plasmid design error-checking is to copy and paste your plasmid into various web servers to predict things about your plasmid, and to have your most experienced cloning guru look over your design—bless them.\nEach failure state could have been caused by one of many errors. Note this diagram does not aim to be comprehensive. The future of cloning Faced with these challenges, some biologists are turning to alternative methods. \u0026ldquo;Cloning-free\u0026rdquo; techniques, outsourcing to companies (Genscript, VectorBuilder, Twist), and \u0026ldquo;cell-free cloning\u0026rdquo; are all attempts to avoid the dreaded cloning.\nBut in many cases, cloning simply must be done. What can be done to make it less potentially painful? Here are a few ideas:\nReducing execution variance: Ideally, you would never have to question whether something went wrong due to human execution error. One development in this vein is benchtop cloning automation, although at $25,000 I’m not sure how appealing it is to labs. For the price of two of those you could hire a human cloning machine—a grad student. Predictive modeling: We need better models to predict how DNA sequences will behave in host organisms. Many already exist, for example transcription and translation prediction in E. coli, and we need to use them more. This could help us identify potential issues before we even start cloning. Better design tools: DNA design software could incorporate more sophisticated rules and predictive models, helping scientists create sequences that are more likely to work. Large language models still lag quite a bit behind humans on answering questions on cloning related tasks (LAB-bench), but I wouldn’t be surprised if that changes soon. Interpretable intermediate states: If we could easily check the correctness of each step, debugging cycles could be shortened. Because of how common it is, I think it is easy to think of cloning as being a procedure with certain success if you don’t mess up (and there are many ways to mess up). But cloning is in part an experiment to see whether a certain host organism will tolerate a certain plasmid.\nAnd hey, if all else fails, there\u0026rsquo;s always that contract with a demon to consider.\nAppendix Some other cloning-related odds and ends that may be of interest:\nYour clonal colonies may not actually be clonal. See “High rates of plasmid cotransformation in E. coli overturn the clonality myth and reveal colony development”\nSam Rodriques on why cloning is hard to automate (tweet):\nE.g. in cloning, everyone has a design workflow they prefer, but any individual plasmid will usually require some modification in the design process. E.g. maybe the sequence isn’t available on Addgene, maybe the restriction site you want to use is methylated, maybe there are repeats, etc. You can write a script to automate design of any individual plasmid (or maybe any individual backbone), but the same script only rarely works across many plasmids. Everything is an edge case.\nDNA quantification is different from instrument to instrument, as much as several fold different. Thanks to Niko McCarty for surfacing this paper.\nQubit is quite different compared to DeNovix and NanoDrop Acknowledgements Thank you to Laura Quinto and Chase Armer for looking over drafts of this piece and providing feedback.\nOther similar essays I enjoyed Too much Tacit (Keoni Gandall) Why is machine learning \u0026lsquo;hard\u0026rsquo;? (S. Zayd Enam) ","date":"Jul 09","permalink":"https://etowahadams.github.io/post/2024-07-17-cloning/","tags":null,"title":"Molecular cloning, or why some biologists become superstitious"},{"categories":["deep learning","projects"],"contents":" Process overview. Skip to process explanation One of the unexpected consequences of attending college in Connecticut has been experiencing autumn twice a year. I watch the trees of New Haven1 change color during late October and early November, then return home in Georgia for fall break and watch the leaves change color again.\nI understood the basic principle behind my double-dipping of fall. In late October, Connecticut is colder and receives less daylight hours than my home in Georgia, triggering trees stop making green chlorophyll and allowing the yellows, oranges, and reds of the other pigments (like xanthophylls, carotenoids, and anthocyanins) to become visible earlier. However, I began to wonder when autumn came to other parts of North America, and the rate that it moved. I have seen fall color forecasts. But what would it look like actually to observe leaf color change across the whole continent? I decided to tackle this question over my winter break.\nHaving been a contributor for several years now, I turned to iNatrualist for answers. For the uninitiated, iNaturalist is a website where users can submit and identify observations of any taxa of life: a social media for naturalists in a sense. I had three qualifications choosing a tree species. It had to have a drastic fall color change, a large range, and many observations. After considering Sugar Maple, (Acer saccharum), Sassafras (Sassafras albidum), Trembling Aspen (Populus tremuloides), among other trees, I finally decided on Red Maple (Acer rubrum), which had the highest observation count.\nResults Figure 1: 2019 research grade observations of A. rubrum with leaves. The color of each marker corresponds to the dominant color of the leaves in the respective observation. Each marker disappears 10 days (frames) after it first appears. At the end of April, observations get green as trees “green up,” and fall colors start appearing at the start of October and spread south. Figure 2 (left): Number of observations of A. rubrum versus week of the year. At the 17th week (around the 4th week of April) there is a sharp spike in the number of observations. I expect this to be when the first leaves first start coming out and everyone is submitting their first A. rubrum observation of the year. Figure 3 (right): Number of observations with leaves versus week of the year. As expected, there is not much difference between figure 2 and figure 3. People usually submit observations with leaves in them. However, notice that in weeks 1-16, there less observations compared to in figure 2, suggesting that the spike of observations at week 17 is because of the emergence of leaves. Still, the peak is remarkably high. Figure 4: 2016, 2017, 2018, and 2019 research grade observations of A. rubrum with leaves. 2015 omitted so that a nice square could be made Figure 5: Number of observations with “fall” colors in each week of the year for 2015-2019. “Fall” color was defined in an HSV color model as colors with a hue (0-360) \u0026gt; 300 or \u0026lt; 60, a saturation (0-1) \u0026gt; 0.09, and a value (0-255) \u0026gt; 150. Peaks are present at week 17, and weeks 41-44. Week 17 is overrepresented because of the number of total observations in that week, but early leaves can be reddish. Interestingly the 2018 fall colors appear to be about two weeks later as a whole compared to 2019. The late 2018 fall was observed by others, and was attributed to higher than average daily low temperatures. Process I painstakingly looked through the images of all 10,000+ observations and wrote down each the color of the leaves in each observation.\nJust kidding.\nLeaf Detection An observation’s images could contain branches, buds, bark, roots, seeds, and any number of other features to help with identification. So, I knew I had to detect if there was a leaf in in the image, and where the leaf was in the image. In other words, I was dealing with object detection. I chose a deep learning approach, You Look Only Once (YOLO), for object detection because of its speed and good documentation.\nUsing the iNatrualist API, I downloaded the images from every 10th page of research grade A. rubrum observations. I created a training set by labeling the leaves in 800 images using LabelImg and trained the model for 3500 batches (around 16 hours on my computer). In hindsight, I could have had even better accuracy if I had created additional classes for other features like branches and seeds, but overall, I am pleased with the performance of the model. There are several instances where it identified very small leaves, getting more sky than leaf, resulting several blue to white dominant color however. See the object detector on Github.\nFigure 6: Leaves detected by YOLOv3 object detector trained on 800 labeled images Leaf Color Once I knew where the leaf was, I expected getting the leaf color to be easy, but it required more experimentation than I expected. At first, I tried to get the color of the leaf without doing any additional manipulation to the leaf sub-image. I used k-means clustering to get the dominant color of the image. I was surprised by how well k = 3 performed, but I thought I could get an even better color if I extracted the foreground.\nFigure 7: Dominant color using k-means clustering. I tried a variety of methods, including creating masks from the largest contour provided by canny edge detection and adaptive thresholding, but with limited success. Eventually I stumbled upon the grabcut algorithm, which performed miles better than anything I had come up with.\nFigure 8: Foreground extraction using mask created with the largest contour found using adaptive thresholding. Not terrible, but also not good. Figure 9: Foreground extraction using grabcut, followed by k-means clustering (k = 2) of the foreground. Much better. Grabcut worked well for leaves with defined edges, but for leaves with less defined leaf edges, sometimes it would cut out the entire foreground, which in case I would get the dominant color of the entire leaf sub-image. When an observation had multiple leaves, I got the dominant color (k = 2) of the dominant colors of each of the leaves. If I were to do it over, I would have combined the extracted leaf foregrounds into the same array, and taken the dominant color of that.\nOnce I got the main image processing pipeline set up, I ran it on the images with leaves in them for all the observations of A. rubrum between 2015-2019. See it on Github.\nCreating the Graphics I initially created a map using leaflet.js but animating the markers was not smooth enough for my liking. Instead, for every day in each year I generated an SVG image. These images were combined into a gif, using Imagemagick, and then converted into a mp4 using ffmpeg. For a future project, I would like to try something with CSS sprite sheet animation, d3.js, or maybe even three.js, but for a long animation like this one, I think the gif was appropriate and is very sharable.\nConcluding Thoughts I found it fascinating to see leaf colors change across eastern North America. Observations are increasing every year so this type of visualization will only get better, although certainly they will be skewed towards certain geographic areas and time periods. Users tend to submit observations when they notice something different, so data mostly comes from periods of transitions. I was happy to see that the late timing of fall colors of 2018 was reflected in figure 5.\nSee this cool map of New Haven trees by Yale Urban Resource Initiative\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":"Jan 09","permalink":"https://etowahadams.github.io/post/2020-01-09-maple-red-leaf/","tags":null,"title":"Leaf color change in 10,950 iNaturalist observations"},{"categories":null,"contents":"","date":"Jan 01","permalink":"https://etowahadams.github.io/articles/","tags":null,"title":"Articles"}]